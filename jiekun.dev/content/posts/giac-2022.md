---
title: "GIAC 2022"
date: 2022-07-23T17:43:07+08:00
type: post
author: jiekun.zhu@shopee.com
category: Kubernetes
comments: true
---

周末蹭公司的门票参到 [GIAC](https://giac.msup.com.cn/2022sz/schedule) 现场学习了两天，受疫情影响，大会临时从华侨城改到了 40 公里外的龙岗，原定早上出发晚上回来的行程直接改成在酒店住两晚。对离得近的同学来说，改期自然是要比改地点好一些，特别感谢 **Shopee Academy** 的同事一直帮忙协调解决了很多问题。

![](../202207-giac/giac_entrance_ticket.jpg)

## 路线规划
GIAC 每天分为上下午场，每场又有 6 个分会场，分别对应不同主题，一个主题下大约有 3-4 场分享，因此想全都到现场听是不可能的。晚上在酒店预先做了点攻略，先把感兴趣的主题圈画出来，再看主题下具体有什么分享。

![](../202207-giac/giac_topics.png)

选择听哪场分享影响因素有很多，例如作为后端工程师，我可能对前端、AI、大数据的内容暂时不感兴趣，如果想去听，那么：
- 要花大量时间预热背景知识，在半天时间内几乎不可能；
- 对团队内这个方向的实践不熟悉，不好对比，没有对比就没办法提出学习改进的思路。

另外因为前几周刚刚返厂维修了膝盖，走路全靠拐杖，要在不同会场之间蠕动还很困难，所以尽可能保持每个半天都待在同一个会场，剩余分享可以在会后看回放学习，只少了提问交流的机会。

综合各种因素和限制，最后选择了学习以下的分享，按照自己的理解重新分类，并从中挑选几个话题进行分享：
- **Database**
    - 快手数据总线 KBus 的设计和实践
    - 云巢：基于 K8s 的数据库云原生服务平台实践
- **服务治理**
    - 基于 dubbo-go 构建跨语言的服务治理平台
    - 无侵入式多重灰度和全链路压力测试
- **Open-source software**
    - 让开源成为公司技术管理的杠杆
- **运维**
    - 腾讯游戏的海量 SRE 运维实践
- **Kubernetes**
    - 云原生跨集群统一算力调度在网易的实践

## 快手 KBus：CDC 解决方案
数据写入 Database、缓存、搜索引擎是很常见的存储方案，在过往的一些项目中，产生数据的一方会独立完成这些操作，先写入某个中间件，再写入某个中间件。引入 Databus（数据总线）之后，产生数据的应用可以专注于完成 Database 的写入，在它的视角里面，数据库写事务成功提交，处理就已经完成了，写入架构可以变成下面这样：

![](../202207-giac/kbus.png)

### Databus 模式

在 Shopee Affiliate 团队，同样也有 Databus 的概念。当用户注册后，一条 “注册” 消息会被发送至 Kafka，不同微服务消费到这条消息后对用户进行审核、打标（Tagging）等操作。跟快手 KBus 的不同之处在于，KBus 本质上是一个 CDC 应用，它的消息代表的是数据变更内容；而 Affiliate 团队的 Databus 是用 Kafka 承载的是一个业务事件消息。

![](../202207-giac/different_databus.png)

两种类型的 Databus 是否可以合并呢？其实业务事件大多数场景下都是一次 Database 操作的结果，例如用户注册可能就是一条 `INSERT` 语句，用户审批可能是一条 `UPDATE` 语句等，Consumer 确实可以通过识别这些 SQL 语句的执行来实现类似功能，但是：
- 如果业务非常复杂，从 100 次 `INSERT` 消息中识别一个事件可能需要筛除掉另外的 99 条 `INSERT` 消息。换句话说，SQL 执行的语义必然不如自定义的业务事件**语义清晰**，简化 Producer 的逻辑即是加重 Consumer 的逻辑；
- 业务应用有能力向 Databus 投递复合的数据。例如一个 User 的数据散落在不同表中，我希望借助 Databus 来更新这个 User 的缓存，单靠某句 `INSERT` 或者 `UPDATE` 的消息不容易完成，需要联合上下文相关的语句才能构建完整的 User 模型。这种场景在 KBus 的模式下就会比较困难，需要 Consumer 来处理；而如果 Producer 投递的就已经是组装好的数据，那 Consumer 侧就只需要 `SET` 操作直接更新缓存即可。

### CDC 工具
在业务的角度来看，如果在使用不同的存储中间件，用**可靠**的 CDC 方案来完成次级存储（Redis、Elasticsearch、Clickhouse）的写入可以提升开发效率，在一些方案设计时，只要体现出业务应用与 Database 的交互，不再关心缓存的写入时机，听起来似乎很诱人。Affiliate 团队当前大部分 Redis 的写入是在业务应用内完成，Elasticsearch 的写入则是利用与 KBus 类似的 CDC 组件来完成。

在确认 KBus 是一个 CDC 平台后，很自然地会将它与我们当前正在使用的 [canal](https://github.com/alibaba/canal) 进行比较。下图来自分享老师的 PPT，可以看到 KBus 内部是多个 Server + Kafka 的模型。

![](../202207-giac/kbus_design.png)

其中，**Meta Server** 存储元信息，不同 Database 的配置应该是依赖它来存储的，而**协调中心**自然是分布式系统中的中心化节点，协调不同 Server 的工作。而 Server 就是伪装成 Database 的 Slave，是真正的 CDC worker。

这套设计跟 canal 很像，canal 中没有 Meta Server，取而代之的是一个 MySQL 或配置文件；协调中心则是直接使用了 ZooKeeper，“协调” 工作通过类似于**抢占**的操作来完成，某个 Server 抢占到了 Database 的使用权，那它就是这个 Database 的 CDC worker。

canal 中 ZooKeeper 同时用于 “协调” 和 “记录位点信息”，KBus 将它们拆到了不同的地方，但不管怎么做，由于 “消息投递至 Kafka” 与 “更新位点（至 ZooKeeper 或 Meta Server）” 不是事务操作，总会出现消息已投递，但未来得及记录进度，Server 就意外不可用的情况，紧接着替代的 Server 就会从历史进度开始获取变更数据信息继续投递。这个问题在 Consumer 视角来看，就是会出现消息重复投递的情况，需要自行实现幂等处理。

![](../202207-giac/kbus_server.png)

在现场我还提了另一个问题，日常 canal 使用中，我们发现如果单纯依赖 “抢占” 逻辑，当出现新的 Database 需要被 CDC worker 处理时，极有可能出现资源分配不均匀的情况，某个 CDC worker 抢占到了大量 Database 目标，而其他 CDC worker 则没有太多负载。KBus 的解决方案是通过考察落在 Server 上的 Database 数量来尽可能保持均衡，这样做一定程度能缓解倾斜的问题，但是由于不同的 Database 使用情况不同，例如有的 Database 可能写操作非常多，而有的 Database 则很闲，几乎不产生数据变更，那实际上 Server 上的 CPU 使用情况仍然会出现倾斜。当然，这个问题优先级可能比较低，况且 KBus 也已经有根据数量来保持平衡的策略，比完全依赖抢占会改善很多。对 CDC 组件来说，保证消息不丢失是最重要的。

## 开源与技术管理
更新中...

## 其他
原本以为临时改了这么远的地点，参会的人会少很多，毕竟也提供了线上直播，但是其实周五当天人还是挺多的，以致于换会场的时候发现没有位置坐了，拄着拐杖听了大半个小时。周六反而人少了一些，可能大家都觉得周五外出摸一天鱼更有利于吸收知识。

![](../202207-giac/giac_entrance.jpeg)

![](../202207-giac/audience.jpeg)

早上急急忙忙起来只吃了个鸡蛋，喝了豆浆，偷了根香蕉就往会场赶，倒是中午的时候听完慢悠悠去用餐比较舒服，但是去晚了就剩养生食品，想着晚上要出去吃，中午就随便糊弄一下也行。

![](../202207-giac/lunch.jpeg)

晚上跟朋友去吃了一顿自助餐，改善一下开会的体验，毕竟坐一整天确实挺累的，不过如果有下一次的话，那大概也还是会去，咱毕竟是去上课不是来接雨水的。

![](../202207-giac/dinner.jpeg)
