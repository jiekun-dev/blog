---
title: 业务核心数据库架构演变——权衡取舍的艺术
type: post
date: 2020-12-21T22:49:07+08:00
author: jiekun.zhu@shopee.com
excerpt: Shopee供应链Logistics Channel Service项目是物流履约链路上的最后一环，连接Shopee物流订单与第三方物流服务(3rd Party Logistics)，几乎所有与3PL的交互都会收拢到LCS服务中。从2019年9月至今，随着上游系统拆分迁移，越来越多3PL进入LCS负责范围；并且由于电商业务在2020年的飞速发展，系统每月的订单量也在陡增，对数据库的要求和压力也随之而来，在短短1年内，数据库架构跟随业务增长进行了多次调整。本文简要回顾了LCS诞生至今的数据库架构变化，并介绍项目组在2020年末落地的分库实践，梳理落地过程中踩过的坑和总结的经验、教训和思考。新方案实践的过程中，有遇到常规库表拆分中共有的问题，也有针对项目场景的“特色”问题，解决问题更多是权衡取舍的过程，在没有完美解决方案的情况下，需要结合业务特性来分析和选择最有利的方式处理。
category: Database
comments: true
archieved: false
---

> Shopee供应链Logistics Channel Service项目是物流履约链路上的最后一环，连接Shopee物流订单与第三方物流服务(3rd Party Logistics)，几乎所有与3PL的交互都会收拢到LCS服务中。
> 
> 从2019年9月至今，随着上游系统拆分迁移，越来越多3PL进入LCS负责范围；并且由于电商业务在2020年的飞速发展，系统每月的订单量也在陡增，对数据库的要求和压力也随之而来，在短短1年内，数据库架构跟随业务增长进行了多次调整。
>
> 本文简要回顾了LCS诞生至今的数据库架构变化，并介绍项目组在2020年末落地的分库实践，梳理落地过程中踩过的坑和总结的经验、教训和思考。
>
> 新方案实践的过程中，有遇到常规库表拆分中共有的问题，也有针对项目场景的“特色”问题，解决问题更多是权衡取舍的过程，在没有完美解决方案的情况下，需要结合业务特性来分析和选择最有利的方式处理。

# Introduction
## 项目起源
LCS是一个基于Python Django框架的项目，业务核心是物流订单的履约过程，包括连接上游和第三方物流服务的创建订单、轨迹与运费更新。在部署上，LCS依据业务所在的市场（印尼、马来西亚、新加坡等）不同，应用层分市场部署，并使用各自市场对应的数据库。在项目起步初期，这些不同市场的数据库共用同一套物理集群，共享xxxxGiB内存和xxxxTiB的磁盘空间，在资源上看，是足以应付初期流量的。

![](../202012-db-partition/lcs_db_single_cluster.png)

随着业务的铺开，共用集群的问题慢慢体现了出来。地区与地区之间存在业务差异，订单数量、订单轨迹比例不同，依赖物理集群自身的资源调度不能满足我们的需要，很多时候：
- 订单量大的地区的数据库读写影响了订单量少的地区的I/O资源
- 轨迹推送QPS远比下单高，大量的轨迹信息读写影响了核心的下单流程

在尽量不影响业务的情况下，我们先选择了将热点地区的数据库拆分到单独的物理集群上，独占资源，保证自身服务稳定的同时，也减少了对其他地区服务的影响。在这个过程中，由于原来的设计使用的是逻辑库，所以切换没有任何的业务改动，在DBA的协助下，可以非常迅速地完成这个操作。

![](../202012-db-partition/lcs_db_multi_cluster.png)

面对轨迹推送更高QPS的问题，我们使用了临时表和消息队列进行削峰，让解析轨迹并绑定到对应订单上的过程延迟发生，整个处理操作更加平滑。问题在于临时表是设计在业务MySQL库中的，业务上，轨迹更新可以缓冲，但是MySQL受到的压力并没有减少。

因此，在前面的物理集群拆分后，我们又启用了TiDB存放临时轨迹数据，尽量降低对MySQL的`INSERT`和`DELETE`操作数量，这样做的好处在于能减少由于`DELETE`操作带来的频繁页分裂与合并，有利于查询性能，以及写操作引起的主从延迟。

![](../202012-db-partition/lcs_db_with_tidb.png)


## 面临挑战
尽管已经做了两次拆分和调整，但是随着订单量的持续上涨，又由于物流订单中的信息繁杂，尤其上游早期未加以限制的各种收发货地址、名称、联系方式，以及跨国运输业务所需要的详细的商品细节，再加上物流轨迹的描述信息，让每个订单的文本类型数据非常多，数据库表的体积容易暴涨。

根据2020年下半年的数据统计，印尼地区的MySQL集群磁盘使用量在短短数个月内已经突破30%，并且随着流量灰度比例上涨，每个月的增量都比以往更大。在灰度结束后，又迎来了下半年几个重要的促销活动时间节点——双9、双11、双12，数据体积进一步增长，仅通过日常的数据清理与数据库压缩已经没有办法维持集群磁盘空间的未来使用了。

# Evolution
DBA团队在历次集群调整的过程中，都一再强调了数据增速过快，磁盘空间规划和使用的问题。而业务产品团队也提出运营数据相关的需求，历史物流订单的数据在未来没有办法直接删除，而需要归档用于统计分析。这也推动了业务研发团队要尽快输出方案，应对更大的单量和不同的产品需求。

## 设计目标
在当前设计下，每个地区使用单个数据库，由于单库的数据体积暴涨，因此将各地区的数据库进行分库势在必行。我们期望新的架构可以：
- **承载未来较长一段时间的数据存储需求**。因为硬件资源有限，很多时候变动并不容易实施，而且重新调整架构也是让DBA和业务研发人员非常痛苦的一件事情
- **同样稳定可靠**。对核心业务链路上的项目而言，稳定性是最重要的
- **保留合理的可扩展性**。重新调整架构很难，但不代表当前架构下不能做业务无感知的横向扩展
- **便于进行数据归档**。来自产品团队的需求，数据也是支撑未来业务设计的核心因素

## 方案设计
### 分库模型
在最初的设计中，我们期望新的分库模型能够很容易完成数据归档的操作，例如可以从一个较粗的粒度将历史数据转移到归档机器上。因此第一个诞生的想法是设计**按月份拆分的数据库**。当前的单库设计——所有的数据都落在不同的哈希分表中，要将历史数据捞出来进行转移是非常麻烦的：
- DBA团队不会访问和维护数据库中的数据，因此业务侧要自己完成归档操作
- 访问和迁移指定时间范围的数据，意味着所有哈希分表的B+树需要发生大量的页合并操作，不利于线上业务的性能

因此，按月分库的想法初衷是想把可以进行归档的数据集中在一起，不管是业务侧想办法迁移，还是DBA能够提供数据库维度的操作，都能简化一定的操作成本。

![](../202012-db-partition/lcs_db_time_partitioning.png)

然而，没有十全十美的方案，如果想要按月份拆分，当月内的数据就会落到同一个数据库中，在未来不可避免会存在各个库的数据量不均匀，热点数据集中等问题。并且随着单月数据量的增大，未来再来处理单库体积，例如将单月的数据再做哈希分库，数据库的结构就变得更为复杂了。

所以，第二个较为容易实施的划分方案自然是一开始就**直接按照哈希分库**，让数据均匀地落到不同的分库中去。但这样又有和单库做数据归档一样的问题——数据均匀分布，没有较粗的粒度可以定位出历史数据，需要依赖应用层一点一点做迁移。

![](../202012-db-partition/lcs_db_hash_partitioning.png)

这时，我们需要考虑在两种方案之间做一定的权衡，选取更适合当前和未来业务的方案：
- **支撑未来业务**：两种方案都降低了单库的数据量，但是按时间分库会在业务进一步增长后，单库体积继续难以管控；哈希分库会更加均匀
- **稳定性**：从部署和使用上，两种方案的稳定性影响因素都比较类似。但是以时间分库可能需要DBA团队的手动维护，包括如何持续未来各个月份的数据库等。我们会更倾向于使用人工操作次数少的方案，哈希分库在搭建完成、交付后更可靠。
- **可扩展性**：按时间分库比较难继续做拆分，例如在未来改成时间+哈希的方案，但是会显著增加复杂度；哈希分库可以考虑直接增加“`% base`”的数量，业务调整更小、更容易落地
- **数据归档**：哈希分库不好处理归档数据，归档操作粒度太细，有应用层的操作开销，引入大量的数据行`DELETE`操作，并且不加以控制资源使用量会影响线上业务；按时间分库如果有DBA支持，可以更方便地完成数据归档，操作难度大幅降低

结合各方面的考量，我们**最终选择了哈希分库的方案**，通过牺牲数据归档操作的便捷性，期望能让业务应用在未来相对更稳定可靠。

### 应用部署架构拆分
在确认分库的模型后，我们又遇到了另一个问题：受限于资源，想要给每个地区市场搭建多套物理集群，让每个哈希的分库都能独享机器，目前机房的剩余数量已经没办法支持。按照单个MySQL集群1主5从的搭配，8个地区、每个地区8个哈希分库，一共需要`6 * 8 * 8 = 384`台物理机，成本是非常高昂的。所以DBA团队能够交付到给业务团队的集群数只有8套，也就是每个地区独享、8个哈希分库共享各自的集群。

由于Python应用的特点，各个Pyhton进程都持有各自的连接池，因此，业务侧持有的数据库连接数为：
- **单进程连接数 * 进程数 * 容器数**

而由于机器资源的问题，不同的8个哈希分库又落在了同一套物理集群上，对主库的机器而言，连接数上限变成了：
- **单进程连接数 * 进程数 * 容器数 * 数据库数**

![](../202012-db-partition/lcs_db_connections.png)

依照当前的业务量进行计算，维持现有架构直连新分库集群将会超过机器设置的连接数上限，而且本身大量的连接数对服务而言也是非常不利的。因此业务侧需要想办法降低连接数峰值，否则新的数据库集群将无法使用。

改造可以从计算的公式入手，降低4个参数中的任意一个都能提供我们想要的效果。其中：
- 数据库数是依据业务数据，从保证未来单库数据体积不超过一定值计算得出的，因此不能再继续调低
- 进程数和连接池连接数的配置是依照业务请求量调整的，降低的话会让单容器的请求处理能力下降

所以我们将尝试在容器数量上做改变，对需要访问核心数据库集群的订单流量与需要访问TiDB的轨迹信息流量进行划分。前面提到过，通常**轨迹流量会是订单流量的数倍**，如果可以将轨迹流量划分到不同的容器上去，那么需要访问核心数据库的容器就只可以大幅降低。以印尼地区为例，轨迹请求数量为下单请求的9倍，参考这个比例，并考虑保护核心的下单业务流程，我们在Nginx层**将原来处理混合下单、轨迹等请求的应用服务进行了拆分**。

拆分后，处理下单等服务的**容器数量降低到原来的25%**，相应地，在连接数计算结果上，数字也缩小了**1/4**。

![](../202012-db-partition/lcs_db_connections_with_different_app.png)

当然，造成连接数过高的问题有很多，解决方案也有有很多，例如用Golang将项目重构、在应用层和数据库集群之间再搭建一套服务用来统一管理连接、使用分库的中间件等。目前对LCS项目而言，重构的开销会比较大，其他的方案也需要单独设计和推动，**应用架构拆分**可能是较为可靠且**最容易落地的方案之一**。

## 业务改造
确认数据库架构方案之后，应用层需要进行调整的内容主要包括：
- 分库依据规则
- 应用数据路由
- 跨库操作补偿

### 分库依据与路由规则
为了避免数据迁移，让新数据库架构能够平滑、可灰度地上线，我们圈定了部分表作为划分进分库的范围，这部分表的**新增数据将会在新分库集群上进行读写，而老数据会在原数据库读写**。

业务团队梳理了对应范围表的读写模型，因为**都依照其中的订单ID进行读写**，毫无疑问**订单ID可以作为分库的依据**，只需要稍加改造，加入一些标志信息，让应用层能区分开：
- 订单ID应该在原数据库还是新的哈希分库
- 订单ID应该落在哪个哈希分库

调整框架内的数据库路由规则比较繁杂，包括将每类表的管理类重写，因为涉及的表比较多，所以也改动了大量的代码，这部分不一一赘述。

### 跨库一致性补偿
由于只圈定了部分表到分库中，那么这部分表与原数据库表的交互就丢失了事务可行性，也就是没办法依赖数据库的事务保证它们一致了。MySQL提供了XA跨库事务操作，但是在实际使用中，使用XA跨库事务有很大的性能开销，并且也需要开发者有对应的知识储备来正确操作。

因此，最终应用层的方案是增加一套检查机制，使得不同库表的操作如果发生异常，也能在异步的检查任务执行完成后，恢复（回退/清理）到一致的状态。

以下单流程为例，我们有以下表落在了不同数据库，但需要严格保持一致性：
- 订单表：在分库中，存有订单单号和订单对应3PL物流单号信息（通过订单单号获取订单信息，包括3PL物流单号）
- 反查表：在旧库中，存有3PL物流单号信息和对应订单单号信息（通过3PL物流单号查订单单号）

> 由于分库依据是订单ID，所以反查表的查询模式（条件只有3PL物流单号）让它没有办法放在分库中，只有带订单ID的查询才能在分库集群上正确被路由，否则需要遍历所有集群Fetch数据。

为了保持反查表和订单表数据的一致，我们在旧库中新增了一个检查表（Checker），这样Checker与反查表能使用事务特性，保证如果反查表需要发生变更，会有对应的Checker被记录下来。

反查表操作结束后，应用层继续操作订单表，如果操作失败，此时**反查表和订单表就会出现数据不一致的情况**。应用层有定期执行的**异步任务**，通过检查Checker信息，**对比当前订单表和反查表是否一致**，如果不一致，则通过**修改反查表记录让它们回到一致的状态**。

![](../202012-db-partition/lcs_db_distributed_data_consistency.png)

以上图为例，`mapping`与`order_info`数据在不同数据库中，通过单库事务保证`mapping`数据与`check_log`数据同时存在，若后续`order_info`数据执行异常，后台任务延迟获取到`check_log`后，对比`mapping`与`order_info`数据，选择是否要对`mapping`进行清理。

**Checker扮演的角色相当于InnoDB中的undo log**，如果事务执行失败，参照其中的内容进行回退或类似的操作，保证最终一致性。

应用层实现数据一致性补偿，是最初提出几个不同方案的其中之一。如引用内容所述，每个分库中都设置对应的反查表，同样可以继续保留数据库事务的可行性，但是因为查询条件不同，需要额外遍历所有分库进行查询的开销。以上，包括不使用XA事务，都是出于不同业务场景、实践成本下的权衡取舍结果。

## 落地上线
### 异常分支验证
在验证上述设计的各类改造时，测试团队也花费了大量的时间进行调试，尤其对于应用层有复杂的事务操作时，**异常情况的分支会比较多**，必须要确认应用层实现的**补偿方案对所有情况都能正确保证跨库数据的一致性**。

### 灰度发布
应用架构、数据库架构的调整，在项目上都引入了非常多的新内容，因此灰度发布需要检查的内容也非常多。

应用架构将部分流量划分到了全新的服务中去，在上线需要谨慎验证新服务的各种中间件连通性，流量处理结果是否与原服务一致，并且轨迹信息数据的发起端在第三方，任何的请求丢失都可能影响到重量和运费计算等核心指标。因为路由划分依赖Nginx，在发布时我们提前部署好应用层的服务，并让Devops团队协助，在接入层节点上按比例灰度线上流量。

数据库架构的调整依赖应用改动，流量由应用层控制，因此灰度也由应用层实施。应用层通过配置开关管理订单ID规则，让特定容器IP、特定订单产生新分库规则的订单ID，其余订单仍保持使用旧订单ID。观察灰度订单的完整履约结果，确认无误后再慢慢扩大灰度比例。

# Conclusion
除非有完美的解决方案，每一次的架构调整背后都是综合考量和权衡取舍的结果。本文中介绍了LCS项目在上线一年以来数据库架构演变过程的重要节点，以及在2020年末进行的分库改造实践、踩坑和总结思考。在新项目架构设计过程中，通常人们都会想设计一套未来数年都能稳定可用的系统，但是由于各种因素的限制，一些方案或是初期成本过高（尽管能减少未来的变更），或是较为激进、不可控，都会影响到最终实施和落地难度，而被排除在外。

架构设计是基于当前、考虑未来、平衡成本的任务，多个方面相互制约，未必都有最优解。在存在短板时能取长补短的系统架构才更有利于快速的业务发展。
